<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?size=28&duration=3000&color=F75C7E&center=true&vCenter=true&width=900&lines=Production-Grade+LangChain+System+Architecture;LLM+Pipelines+%7C+RAG+%7C+Vector+Search+%7C+Tool+Execution;Modular+Design+of+Composable+LLM+Infrastructure" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python" />
  <img src="https://img.shields.io/badge/LangChain-Core-orange?style=for-the-badge" />
  <img src="https://img.shields.io/badge/LLM-OpenAI_+_Gemini-black?style=for-the-badge" />
  <img src="https://img.shields.io/badge/VectorDB-Chroma_+_Pinecone-purple?style=for-the-badge" />
  <img src="https://img.shields.io/badge/RAG-Implemented-success?style=for-the-badge" />
</p>

---

# ğŸ§  Overview

This repository demonstrates a **production-grade modular implementation of LangChain architecture**, covering:

- Multi-provider LLM integration  
- Prompt engineering & structured parsing  
- LCEL runnable pipelines  
- Retrieval-Augmented Generation (RAG)  
- Vector storage & semantic search  
- Tool-augmented LLM workflows  

Each component is implemented independently to deeply understand system behavior before integrating into full-scale LLM pipelines.

---

# ğŸ¯ Why This Project

Modern LLM applications require more than just model calls.  
They require:

- Retrieval-grounded reasoning  
- Deterministic schema validation  
- Composable execution graphs  
- Tool-driven decision-making  
- Multi-model abstraction  

This project demonstrates **system-level understanding of scalable LLM infrastructure design.**

---

# âš™ï¸ System Architecture

## ğŸŸ¢ Standard LLM Pipeline

```
User Input
    â†“
Prompt Template
    â†“
Chat Model (OpenAI / Gemini)
    â†“
Output Parser
    â†“
Structured Response
```

---

## ğŸ”µ Retrieval-Augmented Generation (RAG)

```
User Query
     â†“
Text Splitter
     â†“
Embeddings
     â†“
Vector Store (Chroma / Pinecone)
     â†“
Retriever
     â†“
LLM + Retrieved Context
     â†“
Grounded Output
```

---

# ğŸ§© Core Implementation Components

## ğŸ”¹ 1. Models

- OpenAI Chat Model integration  
- Gemini model integration  
- Provider-based configuration  
- Temperature & invocation control  
- Multi-provider abstraction layer  

---

## ğŸ”¹ 2. Prompts

- PromptTemplate  
- ChatPromptTemplate  
- Message objects  
- MessagePlaceholder  
- JSON-based templates  
- Chat history handling  
- Dynamic prompt utilities  

---

## ğŸ”¹ 3. Structured Output

- TypedDict schema parsing  
- Pydantic validation  
- JSON schema enforcement  
- Deterministic output guarantees  

---

## ğŸ”¹ 4. Output Parsers

- StrOutputParser  
- JSONOutputParser  
- StructuredOutputParser  
- PydanticOutputParser  

---

## ğŸ”¹ 5. Chains

- Simple Chain  
- Sequential Chain  
- Parallel Chain  
- Conditional Chain  

---

## ğŸ”¹ 6. Runnables (LCEL)

- RunnableSequence  
- RunnableParallel  
- RunnablePassthrough  
- RunnableLambda  
- RunnableBranch  

### Composable Pattern

```python
chain = prompt | model | parser
response = chain.invoke({"input": "example"})
```

LCEL enables clean functional composition and scalable pipeline construction.

---

## ğŸ”¹ 7. Document Loaders

- TextLoader  
- PyPDFLoader  
- DirectoryLoader  
- WebBaseLoader  
- CSVLoader  

Supports ingestion of structured and unstructured external data sources.

---

## ğŸ”¹ 8. Text Splitters

- Character-based splitter  
- RecursiveCharacterTextSplitter  
- Token-aware chunking  

Optimized for embedding efficiency and retrieval quality.

---

## ğŸ”¹ 9. Vector Stores

- ChromaDB integration  
- Pinecone integration  
- Embedding persistence  
- Similarity search  

---

## ğŸ”¹ 10. Retrievers

- VectorStore â†’ Retriever conversion  
- Similarity-based context fetching  
- RAG experimentation notebook  

---

## ğŸ”¹ 11. Tools

- Built-in tool usage  
- Custom tool creation  
- Tool calling integration  
- Tool-driven reasoning workflows  

---

# ğŸ§ª Example Use Cases

- ğŸ“„ Document-based RAG chatbot  
- ğŸ” Semantic search over PDFs  
- ğŸ§¾ Structured information extraction  
- ğŸ¤– Tool-enabled research assistant  
- ğŸ§  Multi-step reasoning pipelines  

---

# ğŸ”¬ Technical Highlights

- Multi-provider LLM abstraction  
- Schema-constrained generation  
- Deterministic output validation  
- Retrieval-grounded reasoning  
- LCEL functional composition  
- Modular system isolation  
- Tool-enabled execution patterns  

---

# ğŸ“‚ Repository Structure

```
Langchain/
â”‚
â”œâ”€â”€ models/
â”œâ”€â”€ prompts/
â”œâ”€â”€ output_parsers/
â”œâ”€â”€ structured_output/
â”œâ”€â”€ chains/
â”œâ”€â”€ runnables/
â”œâ”€â”€ document_loaders/
â”œâ”€â”€ text_splitters/
â”œâ”€â”€ vector_stores/
â”œâ”€â”€ retrievers/
â”œâ”€â”€ tools/
â””â”€â”€ notebooks/
```

---

# ğŸš€ Installation

```bash
git clone https://github.com/your-username/Langchain.git
cd Langchain
pip install -r requirements.txt
```

### Environment Variables

```
OPENAI_API_KEY=
GOOGLE_API_KEY=
PINECONE_API_KEY=
```

---

# ğŸ“Š Architectural Emphasis

âœ” Modular component isolation  
âœ” Explicit separation of prompt, model, parser  
âœ” Composable runnable pipelines  
âœ” Retrieval-grounded generation  
âœ” Schema-constrained outputs  
âœ” Tool-driven execution patterns  

---

# ğŸ“Œ Future Enhancements

- LangGraph integration  
- Streaming responses  
- Agent memory systems  
- Evaluation & benchmarking layer  
- Production deployment setup  

---

# ğŸ‘©â€ğŸ’» Author

Built to demonstrate deep understanding of modern LLM application architecture and scalable AI system design.

---
